{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import os\n",
    "\n",
    "LATEX = (\n",
    "    True if os.getenv(\"EXPORT_LATEX\") else False\n",
    ")  # Whether to produce output (figures and tables) for Latex\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import seaborn as sns\n",
    "\n",
    "# sns.set_palette('deep')\n",
    "cmap = sns.color_palette(\"deep\", as_cmap=True)\n",
    "cmap[1], cmap[3] = cmap[3], cmap[1]  # Swap orange and red for paper\n",
    "sns.set_palette(cmap)\n",
    "\n",
    "if LATEX:\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"text.usetex\": True,\n",
    "            \"text.latex.preamble\": r\"\\usepackage{lmodern}\",\n",
    "            \"font.family\": \"sans-serif\",\n",
    "            \"font.size\": 8,\n",
    "            \"figure.autolayout\": True,  # tight layout\n",
    "            \"figure.figsize\": (3.26836, 2.5),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dateutil\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "from pyproj import CRS\n",
    "import pytz\n",
    "\n",
    "import_crs = CRS.from_epsg(4326)\n",
    "tz = pytz.timezone(\"Europe/Berlin\")\n",
    "\n",
    "\n",
    "def import_gps(file: str) -> gp.GeoDataFrame:\n",
    "    df = pd.read_csv(file, parse_dates=[\"loggingTime(txt)\"])\n",
    "    # Epoch time is UTC timezone\n",
    "    df[\"Timestamp\"] = pd.to_datetime(\n",
    "        df[\"locationTimestamp_since1970(s)\"], unit=\"s\", origin=\"unix\", utc=True\n",
    "    ).dt.tz_convert(tz)\n",
    "    gdf = gp.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gp.points_from_xy(\n",
    "            df[\"locationLongitude(WGS84)\"], df[\"locationLatitude(WGS84)\"]\n",
    "        ),\n",
    "        crs=import_crs,\n",
    "    )\n",
    "    gdf = gdf.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def import_offline_finding(file: str) -> gp.GeoDataFrame:\n",
    "    df = pd.read_csv(\n",
    "        file,\n",
    "        parse_dates=[\"Date Published\", \"Timestamp\"],\n",
    "        date_parser=dateutil.parser.isoparse,\n",
    "    )\n",
    "    df[\"Timestamp\"] = df[\"Timestamp\"].dt.tz_convert(tz)\n",
    "    df[\"Date Published\"] = df[\"Date Published\"].dt.tz_convert(tz)\n",
    "    df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    gdf = gp.GeoDataFrame(\n",
    "        df, geometry=gp.points_from_xy(df.Longitude, df.Latitude), crs=import_crs\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def clip_offline_finding(df_of, df_gps):\n",
    "    start = df_gps[\"Timestamp\"].min()\n",
    "    end = df_gps[\"Timestamp\"].max()\n",
    "    mask = (df_of[\"Timestamp\"] >= start) & (df_of[\"Timestamp\"] <= end)\n",
    "    return df_of[mask].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def project_map(gdf):\n",
    "    return gdf.to_crs(epsg=3857)\n",
    "\n",
    "\n",
    "def project_original(gdf):\n",
    "    return gdf.to_crs(epsg=4326)\n",
    "\n",
    "\n",
    "def interpolate_gps(gps_df, of_df):\n",
    "    # Store old CRS\n",
    "    crs = gps_df.crs\n",
    "    gps_time = gps_df[\"Timestamp\"]\n",
    "    of_time = of_df[\"Timestamp\"]\n",
    "    time = gps_time.append(of_time, ignore_index=True)\n",
    "    gps_interpolate = pd.DataFrame(\n",
    "        {\"Timestamp\": time, \"x\": gps_df[\"geometry\"].x, \"y\": gps_df[\"geometry\"].y}\n",
    "    )\n",
    "    gps_interpolate = gps_interpolate.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    gps_interpolate.index = gps_interpolate[\"Timestamp\"]\n",
    "    del gps_interpolate[\"Timestamp\"]\n",
    "    gps_interpolate = gps_interpolate.interpolate(method=\"time\", axis=0)\n",
    "    gps_interpolate = gps_interpolate.drop(gps_time)\n",
    "    gps_interpolate_gs = gp.GeoDataFrame(\n",
    "        gps_interpolate.index,\n",
    "        geometry=gp.points_from_xy(gps_interpolate[\"x\"], gps_interpolate[\"y\"]),\n",
    "        crs=crs,\n",
    "    )\n",
    "    return gps_interpolate_gs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GPS traces\n",
    "gps_files = {\n",
    "    \"Walking\": \"data/gps/Walking-2020-07-29_11_16_05_iPhone-8.csv\",\n",
    "    \"Train\": \"data/gps/Train-2020-07-29_10_40_01_iPhone-8.csv\",\n",
    "    \"Restaurant\": \"data/gps/Restaurant-2020-07-29_12_11_49_iPhone-8.csv\",\n",
    "    \"Car\": \"data/gps/Car_2020-08-23_iPhone-8.csv\",\n",
    "}\n",
    "gps_traces = {name: import_gps(file) for name, file in gps_files.items()}\n",
    "\n",
    "# Preprocess raw OF traces\n",
    "of_files = []\n",
    "for root, dirs, files in os.walk(\"data/of_raw\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            of_files.append(os.path.join(root, file))\n",
    "if len(of_files) > 0:\n",
    "    # Import all traces in single DataFrame\n",
    "    of_trace = pd.concat(\n",
    "        [import_offline_finding(of_file) for of_file in of_files], ignore_index=True\n",
    "    )\n",
    "    of_trace = of_trace.drop(columns=[\"Unnamed: 0\"])\n",
    "    # For each GPS trace, extract offline finding reports based on start and end time\n",
    "    of_traces = {\n",
    "        name: clip_offline_finding(of_trace, gps_trace)\n",
    "        for name, gps_trace in gps_traces.items()\n",
    "    }\n",
    "    # Store clipped data\n",
    "    for name, of_trace in of_traces.items():\n",
    "        of_trace.to_csv(f\"data/of/{name}.csv\", index=False)\n",
    "\n",
    "# Import OF traces\n",
    "of_files = {\n",
    "    \"Walking\": \"data/of/Walking.csv\",\n",
    "    \"Train\": \"data/of/Train.csv\",\n",
    "    \"Restaurant\": \"data/of/Restaurant.csv\",\n",
    "    \"Car\": \"data/of/Car.csv\",\n",
    "}\n",
    "of_traces = {name: import_offline_finding(file) for name, file in of_files.items()}\n",
    "\n",
    "# Compute interpolated GPS traces based on time index of offline finding reports\n",
    "gps_interpolated_traces = {\n",
    "    name: interpolate_gps(gps_trace, of_traces[name])\n",
    "    for name, gps_trace in gps_traces.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import distance\n",
    "\n",
    "\n",
    "def gps_duration(gdf):\n",
    "    return gdf[\"Timestamp\"].max() - gdf[\"Timestamp\"].min()\n",
    "\n",
    "\n",
    "def gps_distance(gdf):\n",
    "    def dist_geodesic(x):\n",
    "        a = x[\"geometry\"]\n",
    "        b = x[\"shifted\"]\n",
    "        if a is None or b is None:\n",
    "            return None\n",
    "        # WARNING: geopy.distance.distance uses (y,x)/(lat, lon) format\n",
    "        return distance.distance((a.y, a.x), (b.y, b.x)).m\n",
    "\n",
    "    gdf2 = gdf.copy()\n",
    "    gdf2[\"shifted\"] = gdf2[\"geometry\"].shift()\n",
    "    gdf2[\"distance\"] = gdf2.apply(dist_geodesic, axis=1)\n",
    "\n",
    "    return gdf2[\"distance\"].sum()\n",
    "\n",
    "\n",
    "def of_reports(df):\n",
    "    return len(df)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    scenario: {\n",
    "        \"Distance\": gps_distance(gps),\n",
    "        \"Duration\": gps_duration(gps),\n",
    "        \"Num. OF reports\": of_reports(of),\n",
    "    }\n",
    "    for scenario, gps, of in zip(\n",
    "        gps_traces.keys(), gps_traces.values(), of_traces.values()\n",
    "    )\n",
    "}\n",
    "stats_df = pd.DataFrame.from_dict(stats, orient=\"index\")\n",
    "\n",
    "if LATEX:\n",
    "    import datetime\n",
    "\n",
    "    def strfdelta(tdelta):\n",
    "        return str(datetime.timedelta(seconds=tdelta.total_seconds()))\n",
    "\n",
    "    stats_df[\"Duration\"] = stats_df[\"Duration\"].apply(strfdelta)\n",
    "    print(stats_df.to_latex(float_format=\"{:0.0f}\".format))\n",
    "else:\n",
    "    display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path estimation (LOWESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def estimate_path(\n",
    "    gdf: gp.GeoDataFrame, lowess_window_size=30, max_frac=1\n",
    ") -> gp.GeoDataFrame:\n",
    "    crs = gdf.crs\n",
    "    frac = min(lowess_window_size / len(gdf), max_frac)\n",
    "\n",
    "    time = gdf[\"Timestamp\"]\n",
    "    x = gdf.geometry.x\n",
    "    y = gdf.geometry.y\n",
    "\n",
    "    lowess_x = sm.nonparametric.lowess(x, time, return_sorted=False, frac=frac)\n",
    "    lowess_y = sm.nonparametric.lowess(y, time, return_sorted=False, frac=frac)\n",
    "\n",
    "    gdf = gp.GeoDataFrame(time, geometry=gp.points_from_xy(lowess_x, lowess_y), crs=crs)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "of_estimated_traces = {\n",
    "    name: estimate_path(of_trace) for name, of_trace in of_traces.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traces(\n",
    "    scenario,\n",
    "    savefig=False,\n",
    "    loc=None,\n",
    "    figsize=None,\n",
    "    force_figsize=True,\n",
    "    with_estimated=True,\n",
    "):\n",
    "    of_trace = project_map(of_traces[scenario])\n",
    "    of_estimated_trace = project_map(of_estimated_traces[scenario])\n",
    "    gps_trace = project_map(gps_traces[scenario])\n",
    "\n",
    "    ax = of_trace.plot(\n",
    "        figsize=figsize, color=\"black\", label=\"Raw OF reports\", marker=\".\"\n",
    "    )\n",
    "    ax.plot(gps_trace.geometry.x, gps_trace.geometry.y, linewidth=2, label=\"GPS trace\")\n",
    "    if with_estimated:\n",
    "        ax.plot(\n",
    "            of_estimated_trace.geometry.x,\n",
    "            of_estimated_trace.geometry.y,\n",
    "            linewidth=2,\n",
    "            label=\"Estimated OF path\",\n",
    "        )\n",
    "\n",
    "    # Get current figsize\n",
    "    if not figsize:\n",
    "        fig = plt.gcf()\n",
    "        figsize = fig.get_size_inches()\n",
    "\n",
    "    if force_figsize:\n",
    "        # Crop figure to respect desired figsize\n",
    "        xlen = figsize[0]\n",
    "        ylen = figsize[1]\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        # xmax-xmin   ymax-ymin\n",
    "        # --------- = ---------\n",
    "        #   xlen        ylen\n",
    "        yminmax = (xmax - xmin) * ylen / xlen\n",
    "\n",
    "        ylim_min, ylim_max = ax.get_ylim()\n",
    "        ylim_mid = ylim_min + (ylim_max - ylim_min) / 2\n",
    "        ax.set_ylim(ylim_mid - 0.5 * yminmax, ylim_mid + 0.5 * yminmax)\n",
    "\n",
    "    ax.legend(loc=loc)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    ctx.add_basemap(ax, attribution=False)\n",
    "\n",
    "    if savefig:\n",
    "        plt.savefig(\n",
    "            f\"plots/traces-{scenario}.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "            metadata={\"CreationDate\": None},\n",
    "        )\n",
    "\n",
    "\n",
    "plot_traces(\n",
    "    \"Walking\",\n",
    "    savefig=LATEX,\n",
    "    figsize=(3.26836, 2.3) if LATEX else None,\n",
    "    loc=\"lower left\",\n",
    ")\n",
    "plot_traces(\n",
    "    \"Restaurant\",\n",
    "    savefig=LATEX,\n",
    "    figsize=(3.26836, 2.3) if LATEX else None,\n",
    "    loc=\"lower right\",\n",
    ")\n",
    "if LATEX:\n",
    "    plot_traces(\n",
    "        \"Train\",\n",
    "        savefig=LATEX,\n",
    "        figsize=(4, 6.69421),\n",
    "        force_figsize=False,\n",
    "        loc=\"lower center\",\n",
    "    )\n",
    "    plot_traces(\n",
    "        \"Car\",\n",
    "        savefig=LATEX,\n",
    "        figsize=(4, 11),\n",
    "        force_figsize=False,\n",
    "        loc=\"lower right\",\n",
    "        with_estimated=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "\n",
    "def distance_geodesic(a_trace, b_trace):\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"distance\": [\n",
    "                # Calculate distance in meters\n",
    "                # WARNING: geopy.distance.distance uses (y,x)/(lat, lon) format\n",
    "                geopy.distance.distance((a.y, a.x), (b.y, b.x)).m\n",
    "                for a, b in zip(a_trace.geometry, b_trace.geometry)\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def distance_geometric(a_trace, b_trace):\n",
    "    # Warning: this produces incorrect results\n",
    "    return project_map(a_trace).distance(project_map(b_trace))\n",
    "\n",
    "\n",
    "def reported_accuracy(of_trace):\n",
    "    df = of_trace[\"Accuracy\"]\n",
    "    df = pd.DataFrame(df.rename(\"distance\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "distances_raw = {\n",
    "    scenario: distance_geodesic(of_trace, gps_interpolated_traces[scenario])\n",
    "    for scenario, of_trace in of_traces.items()\n",
    "}\n",
    "\n",
    "distances_estimated = {\n",
    "    scenario: distance_geodesic(of_estimated_trace, gps_interpolated_traces[scenario])\n",
    "    for scenario, of_estimated_trace in of_estimated_traces.items()\n",
    "}\n",
    "\n",
    "distances_reported_accuracy = {\n",
    "    scenario: reported_accuracy(of_trace) for scenario, of_trace in of_traces.items()\n",
    "}\n",
    "\n",
    "# Put all results in one data frame\n",
    "def tidify(scenario, df, measurement):\n",
    "    df[\"i\"] = df.index\n",
    "    df[\"measurement\"] = measurement\n",
    "    df[\"scenario\"] = scenario\n",
    "    return df\n",
    "\n",
    "\n",
    "df_raw = [\n",
    "    tidify(scenario, df, \"Raw Distance\") for scenario, df in distances_raw.items()\n",
    "]\n",
    "df_estimated = [\n",
    "    tidify(scenario, df, \"Estimated Path\")\n",
    "    for scenario, df in distances_estimated.items()\n",
    "]\n",
    "df_reported = [\n",
    "    tidify(scenario, df, \"Reported Accuracy\")\n",
    "    for scenario, df in distances_reported_accuracy.items()\n",
    "]\n",
    "df = pd.concat(df_raw + df_estimated + df_reported, axis=0, ignore_index=True)\n",
    "\n",
    "mean = df.groupby([\"scenario\", \"measurement\"])[\"distance\"].mean().unstack()\n",
    "mean[\"Improvement\"] = mean[\"Raw Distance\"] / mean[\"Estimated Path\"]\n",
    "\n",
    "if LATEX:\n",
    "    print(\n",
    "        mean.to_latex(\n",
    "            columns=[\n",
    "                \"Reported Accuracy\",\n",
    "                \"Raw Distance\",\n",
    "                \"Estimated Path\",\n",
    "                \"Improvement\",\n",
    "            ],\n",
    "            float_format=\"{:0.1f}\".format,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    display(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting delay\n",
    "We measure the delay between generating and uploading a location report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publication_delay_min(df, value: str):\n",
    "    df[value] = df[\"Date Published\"] - df[\"Timestamp\"]\n",
    "    df[value] = df[value].apply(lambda x: x.total_seconds() / 60.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_cdf(df, value: str):\n",
    "    # CDF calculation from https://stackoverflow.com/a/54317197\n",
    "    cdf = (\n",
    "        df.groupby(value)[value]\n",
    "        .agg(\"count\")\n",
    "        .pipe(pd.DataFrame)\n",
    "        .rename(columns={value: \"frequency\"})\n",
    "    )\n",
    "    cdf[\"pdf\"] = cdf[\"frequency\"] / sum(cdf[\"frequency\"])\n",
    "    cdf[\"cdf\"] = cdf[\"pdf\"].cumsum()\n",
    "    cdf = cdf.reset_index()\n",
    "    return cdf\n",
    "\n",
    "\n",
    "value = \"Publication Delay [min]\"\n",
    "\n",
    "# Plots for individual scenarios\n",
    "# _, ax = plt.subplots(1, 1, figsize=(4.1, 2.2))\n",
    "# for scenario, df in of_traces.items():\n",
    "#    dfa = publication_delay_min(df, value)\n",
    "#    cdf = calculate_cdf(dfa, value)\n",
    "#    cdf.plot(ax=ax, x=value, y='cdf', label=scenario)\n",
    "\n",
    "df = pd.concat(of_traces.values())\n",
    "df = publication_delay_min(df, value)\n",
    "cdf = calculate_cdf(df, value)\n",
    "\n",
    "ax = cdf.plot(x=value, y=\"cdf\", label=\"CDF\", legend=None, figsize=(3.26836, 1.4))\n",
    "median = df[value].median()\n",
    "\n",
    "ax.axvline(median, label=f\"Median: {median:.1f} min\", color=\"black\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(axis=\"y\")\n",
    "ax.set_xlim(0, 3 * 60)\n",
    "ax.set_xlabel(\"Publication delay (min)\")\n",
    "if LATEX:  # save plot to file\n",
    "    plt.savefig(\n",
    "        \"plots/reporting-delay.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0.01,\n",
    "        metadata={\"CreationDate\": None},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Top Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resamping and Clustering Location Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "# PARAMETERS\n",
    "radius = 50  # in meters\n",
    "resample_interval = \"20min\"\n",
    "min_samples = 6  # number of (resampled) samples necessary to form a cluster\n",
    "\n",
    "# 1. run with private data (don't resample)\n",
    "# X = import_offline_finding(\"data/of_raw/top_locations.csv\")\n",
    "# resample_interval = None\n",
    "# 2. run with anonymized data\n",
    "X = import_offline_finding(\"data/of/top_locations_anonymized.csv\")\n",
    "\n",
    "X.index = X[\"Timestamp\"]  # index is generation time of report\n",
    "X = X[[\"Latitude\", \"Longitude\"]]\n",
    "display(f\"original length {len(X)}\")\n",
    "if resample_interval:\n",
    "    X = X.resample(resample_interval, origin=\"start_day\").mean().dropna()\n",
    "    display(f\"resampled length {len(X)} (resampling interval: {resample_interval})\")\n",
    "X = gp.GeoDataFrame(\n",
    "    X, geometry=gp.points_from_xy(X[\"Longitude\"], X[\"Latitude\"]), crs=import_crs\n",
    ")\n",
    "\n",
    "# We need to precompute distances for DBSCAN as Eucledian distance does not work for lon/lat\n",
    "def geodistance(a, b):\n",
    "    return geopy.distance.distance((a[1], a[0]), (b[1], b[0])).m\n",
    "\n",
    "\n",
    "x = np.array(X[[\"Longitude\", \"Latitude\"]])\n",
    "Xd = metrics.pairwise_distances(x, metric=geodistance)\n",
    "\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=radius, min_samples=min_samples, metric=\"precomputed\").fit(Xd)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# Store cluster information in original data frame\n",
    "X[\"IsCoreSample\"] = core_samples_mask\n",
    "X[\"ClusterId\"] = labels\n",
    "X[\"Date\"] = X.index.to_series().dt.date\n",
    "# Compute cluster statistics\n",
    "C = (\n",
    "    X[X[\"ClusterId\"] != -1]\n",
    "    .groupby([\"ClusterId\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"Longitude\": \"mean\",\n",
    "            \"Latitude\": \"mean\",\n",
    "            \"Date\": pd.Series.nunique,\n",
    "            \"geometry\": \"count\",\n",
    "        }\n",
    "    )\n",
    "    .rename(columns={\"geometry\": \"Samples\", \"Date\": \"Days visited\"})\n",
    "    .sort_values(\"Samples\", ascending=False)\n",
    ")\n",
    "C[\"Rank\"] = C.reset_index().index + 1  # TOP-X location (1-indexed)\n",
    "if resample_interval:\n",
    "    C[\"DwellTime\"] = C[\"Samples\"] * pd.Timedelta(\n",
    "        resample_interval\n",
    "    )  # estimate dwell time based on resampling interval\n",
    "C = C.sort_index()\n",
    "C = gp.GeoDataFrame(\n",
    "    C, geometry=gp.points_from_xy(C[\"Longitude\"], C[\"Latitude\"], crs=import_crs)\n",
    ")\n",
    "display(\"Cluster statistics:\")\n",
    "display(C)\n",
    "\n",
    "# Plot everything on a map\n",
    "# Define colors: black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "    # Core samples\n",
    "    xy = X[(X[\"ClusterId\"] == k) & X[\"IsCoreSample\"]]\n",
    "    if len(xy) > 0:\n",
    "        project_map(xy).plot(ax=ax, color=col, marker=\"o\", edgecolor=\"k\")\n",
    "    # Other cluster samples\n",
    "    xy = X[(X[\"ClusterId\"] == k) & ~X[\"IsCoreSample\"]]\n",
    "    if len(xy) > 0:\n",
    "        project_map(xy).plot(ax=ax, color=col, marker=\".\", edgecolor=\"k\")\n",
    "    # Cluster centers\n",
    "    xy = C[C.index == k]\n",
    "    if len(xy) > 0:\n",
    "        project_map(xy).plot(\n",
    "            ax=ax,\n",
    "            color=col,\n",
    "            label=f\"{int(xy.Rank)} ({int(xy.Samples)} samples)\",\n",
    "            marker=\"X\",\n",
    "            edgecolor=\"k\",\n",
    "            markersize=200,\n",
    "        )\n",
    "\n",
    "ctx.add_basemap(ax, attribution=False)\n",
    "\n",
    "ax.legend(title=\"Cluster rank\")  # Indicate cluster IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymize trace\n",
    "\n",
    "**Important:** You need to run the previous cell once before so that `X` contains the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "if os.path.exists(\"data/of_raw/top_locations.csv\"):\n",
    "\n",
    "    Y = import_offline_finding(\"data/of_raw/top_locations.csv\")\n",
    "    Y[\"ClusterId\"] = X[\"ClusterId\"].reset_index(drop=True)\n",
    "\n",
    "    # Store old coordinates\n",
    "    # Y['OldLatitude'], Y['OldLongitude'] = Y['Latitude'], Y['Longitude']\n",
    "\n",
    "    def transform(lat, lon, dlat, dlon, r_earth=6378e3):\n",
    "        lat = lat + (dlat / r_earth) * (180 / np.pi)\n",
    "        lon = lon + (dlon / r_earth) * (180 / np.pi) / np.cos(lat * np.pi / 180)\n",
    "        return lat, lon\n",
    "\n",
    "    # City center of London\n",
    "    c_lat = 51.509865\n",
    "    c_lon = -0.118092\n",
    "    # Radius in meters\n",
    "    r = 10000\n",
    "\n",
    "    def random_locations(center_lat, center_lon, radius, k):\n",
    "        dlat = np.array(random.choices(range(-radius, radius), k=k))\n",
    "        dlon = np.array(random.choices(range(-radius, radius), k=k))\n",
    "        return transform(center_lat, center_lon, dlat, dlon)\n",
    "\n",
    "    # Randomly relocate clusters\n",
    "    cids = Y[\"ClusterId\"].unique()\n",
    "    clusters = Y.groupby(\"ClusterId\").first()\n",
    "    # Generate new points\n",
    "    new_lat, new_lon = random_locations(c_lat, c_lon, r, len(cids))\n",
    "    # Compute Lat/Lon offsets\n",
    "    dlat = new_lat - clusters[\"Latitude\"]\n",
    "    dlon = new_lon - clusters[\"Longitude\"]\n",
    "    dlon = dict(zip(cids, dlon))\n",
    "    dlat = dict(zip(cids, dlat))\n",
    "\n",
    "    Y[\"dlat\"] = Y[\"ClusterId\"].replace(dlat)\n",
    "    Y[\"dlon\"] = Y[\"ClusterId\"].replace(dlon)\n",
    "    Y[\"Latitude\"] = Y[\"Latitude\"] + Y[\"dlat\"]\n",
    "    Y[\"Longitude\"] = Y[\"Longitude\"] + Y[\"dlon\"]\n",
    "\n",
    "    # Randomize individual noise points\n",
    "    noise = Y[Y[\"ClusterId\"] == -1]\n",
    "    new_lat, new_lon = random_locations(c_lat, c_lon, r, len(noise))\n",
    "    Y.loc[Y[\"ClusterId\"] == -1, \"Latitude\"] = new_lat\n",
    "    Y.loc[Y[\"ClusterId\"] == -1, \"Longitude\"] = new_lon\n",
    "\n",
    "    Y = gp.GeoDataFrame(\n",
    "        Y, geometry=gp.points_from_xy(Y[\"Longitude\"], Y[\"Latitude\"]), crs=import_crs\n",
    "    )\n",
    "\n",
    "    ax = project_map(Y).plot(figsize=(10, 10))\n",
    "    ctx.add_basemap(ax, attribution=False)\n",
    "\n",
    "    # Drop transformation fields and store anonymized trace\n",
    "    Y = Y.drop(columns=[\"ClusterId\", \"dlat\", \"dlon\", \"geometry\"])\n",
    "    if not os.path.exists(\"data/of/top_locations_anonymized.csv\"):\n",
    "        Y.to_csv(\"data/of/top_locations_anonymized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input true locations here\n",
    "Gdf = pd.read_csv(\"data/of/true_top_locations_anonymized.csv\")\n",
    "G = gp.GeoDataFrame(\n",
    "    Gdf, geometry=gp.points_from_xy(Gdf[\"Longitude\"], Gdf[\"Latitude\"], crs=import_crs)\n",
    ")\n",
    "G.index = G[\"ClusterId\"]\n",
    "G = G.sort_values(\"Rank\")\n",
    "C = C.sort_values(\"Rank\")\n",
    "\n",
    "# Calculate distance cluster center\n",
    "C[\"Accuracy\"] = distance_geodesic(C, G)\n",
    "C[\"Type\"] = G[\"Description\"]\n",
    "\n",
    "if LATEX:\n",
    "    print(\n",
    "        C.to_latex(\n",
    "            columns=[\n",
    "                \"Type\",\n",
    "                \"Rank\",\n",
    "                \"Accuracy\",\n",
    "                \"Samples\",\n",
    "                \"Days visited\",\n",
    "                \"DwellTime\",\n",
    "            ],\n",
    "            index=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    display(\"Cluster statistics with distance\")\n",
    "    display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot dwell times of top locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = X[X[\"ClusterId\"] != -1].copy()\n",
    "H[\"Hour of day\"] = H.index.to_series().dt.hour\n",
    "\n",
    "replace_dict = pd.Series(C[\"Type\"], index=C.index).to_dict()\n",
    "replace_rank_dict = pd.Series(C[\"Rank\"], index=C.index).to_dict()\n",
    "H[\"Top Location\"] = H[\"ClusterId\"].replace(replace_dict)\n",
    "H[\"Rank\"] = H[\"ClusterId\"].replace(replace_rank_dict)\n",
    "H = H.sort_values(\"Rank\")\n",
    "\n",
    "plt.figure(figsize=(3.26836, 1.6))\n",
    "sns.histplot(\n",
    "    data=H,\n",
    "    x=\"Hour of day\",\n",
    "    y=\"Top Location\",\n",
    "    bins=24,\n",
    "    multiple=\"dodge\",\n",
    "    hue=\"Top Location\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "if LATEX:\n",
    "    plt.savefig(\n",
    "        \"plots/top-locations.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0.01,\n",
    "        metadata={\"CreationDate\": None},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "of-venv",
   "language": "python",
   "name": "of-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
